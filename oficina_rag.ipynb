{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ampliando o conhecimento de LLMs com dados externos\n",
    "\n",
    "Este notebook é um material de apoio para a oficina, apresentando um exemplo de implementação de uma arquitetura usando a abordagem Retrieval-Augmented Generation (RAG). O objetivo é demonstrar como é possível ampliar o conhecimento de Large Language Models (LLMs) a partir de documentos proprietários, melhorando a precisão nas respostas e reduzindo possíveis alucinações sobre temas específicos. \n",
    "\n",
    "O notebook possui as seguintes seções: </br>\n",
    "1 - Importação de bibliotecas e setup do ambiente </br>\n",
    "2 - Exemplo de interação com um LLM pré-treinado </br>\n",
    "3 - Exemplo de criação de uma vector store, possibilitando buscas semânticas em documentos </br>\n",
    "4 - Exemplo de interação com um LLM pré-treinado através de uma arquitetura RAG </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Importação de bibliotecas e setup do ambiente\n",
    "\n",
    "Importação das bibliotecas necessárias para executar este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Exemplo de interação com um LLM pré-treinado\n",
    "\n",
    "Neste exemplo, primeiramente avaliamos o conhecimento do modelo pré-treinado \"gpt-4o-mini\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, avaliamos o conhecimento do modelo sobre Alan Turing. Podemos observar que o modelo responde sobre quem é Alan Turing sem maiores dificuldades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alan Turing foi um matemático, lógico, filósofo e criptógrafo britânico, amplamente reconhecido como um dos pais da ciência da computação moderna. Nascido em 23 de junho de 1912, em Londres, Turing fez contribuições significativas para a teoria da computação, incluindo a definição da \"máquina de Turing\", um conceito fundamental que ajuda a entender o que significa calcular.\\n\\nDurante a Segunda Guerra Mundial, Turing trabalhou para os serviços de inteligência britânicos no Bletchley Park, onde desempenhou um papel crucial na decodificação de mensagens criptografadas dos nazistas, particularmente aquelas codificadas pela máquina Enigma. Seu trabalho foi vital para a vitória dos Aliados e ajudou a encurtar a guerra.\\n\\nAlém de suas contribuições à criptografia e à matemática, Turing também é lembrado por seu trabalho em inteligência artificial e pela proposta do \"Teste de Turing\", um critério para avaliar a inteligência de máquinas.\\n\\nTuring enfrentou discriminação devido à sua homossexualidade, que era ilegal no Reino Unido na época. Em 1952, após ser processado por \"indecência grosseira\", ele foi submetido a tratamento hormonal forçado. Turing morreu em 7 de junho de 1954, em circunstâncias que foram consideradas suicídio.\\n\\nO reconhecimento póstumo de seu trabalho e suas contribuições à ciência e à sociedade têm aumentado ao longo dos anos, e ele se tornou uma figura emblemática tanto na história da computação quanto nos direitos LGBTQ+. Em 2013, a Rainha Elizabeth II concedeu-lhe um perdão real póstumo.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Você é um assistente para responder perguntas\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quem é Alan Turing?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porém, ao ser questionado sobre uma pessoa menos conhecida, o modelo afirma que não possui conhecimentos suficientes para responder a pergunta. Isso ocorre porque o modelo não adquiriu esse conhecimento durante o seu treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wesllei Heckler é uma figura pública conhecida no Brasil, principalmente por seu trabalho no mundo do marketing digital e como influenciador nas redes sociais. Ele se destaca por compartilhar dicas sobre empreendedorismo, estratégias de venda e como alavancar negócios online. No entanto, como as informações sobre figuras públicas podem mudar rapidamente, recomendo verificar fontes atualizadas para obter os detalhes mais recentes sobre ele. Se você tiver uma pergunta mais específica, sinta-se à vontade para perguntar!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Você é um assistente para responder perguntas\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quem é Wesllei Heckler?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma possível estratégia para contornar esse problema é fornecer dados sobre a pessoa desconhecida. Desta forma, o modelo consegue responder a pergunta normalmente, pois possui uma excelente habilidade de localizar informações em textos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wesllei Heckler é doutorando no Programa de Pós-Graduação em Computação Aplicada (PPGCA) da Universidade do Vale do Rio dos Sinos (UNISINOS). Ele possui um Mestrado em Computação Aplicada na mesma universidade, obtido em 2022, e uma graduação em Ciência da Computação pela Universidade Feevale, concluída em 2019. Atua como Cientista de Dados, desenvolvendo soluções por meio da Ciência de Dados e Inteligência Artificial, com experiência no desenvolvimento, validação e monitoramento de modelos de Machine Learning. Além disso, Wesllei é pesquisador voluntário no Laboratório de Computação Móvel (Mobilab) da UNISINOS, onde participa de pesquisas que aplicam tecnologias na área da saúde, com um foco especial em Machine Learning. Atualmente, ele investiga modelos de Machine Learning e Inteligência Artificial Generativa voltados para a Saúde Mental, sendo seus principais interesses de pesquisa em Data Science, Machine Learning e tecnologia aplicada à Saúde Mental.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"\"\"\n",
    "        Você é um assistente para responder perguntas. \n",
    "        A cada interação, você receberá um contexto e uma pergunta.\n",
    "        Utilize o contexto para responder a pergunta.\n",
    "     \"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "        Contexto:\n",
    "        Wesllei é doutorando no Programa de Pós-Graduação em Computação Aplicada (PPGCA) da Universidade do Vale do Rio dos Sinos\n",
    "        (UNISINOS). Possui Mestrado em Computação Aplicada na mesma universidade (2022) e graduação em Ciência da Computação pela\n",
    "        Universidade Feevale (2019). Atua como Cientista de Dados, desenvolvendo soluções de dados através do uso de Ciência de Dados e\n",
    "        Inteligência Artificial, com experiência no desenvolvimento, validação e monitoramento de modelos de Machine Learning. Wesllei\n",
    "        também é pesquisador voluntário no Laboratório de Computação Móvel (Mobilab) da Universidade do Vale do Rio dos Sinos\n",
    "        (UNISINOS), onde colabora em pesquisas que envolvem a aplicação de tecnologias na área da saúde, principalmente envolvendo\n",
    "        Machine Learning. Atualmente pesquisa modelos de Machine Learning e Inteligência Artificial Generativa para a área de Saúde\n",
    "        Mental. Seus principais interesses de pesquisa são Data Science, Machine Learning e tecnologia aplicados à Saúde Mental. \n",
    "     \n",
    "        Pergunta:\n",
    "        Quem é Wesllei Heckler?\n",
    "     \"\"\"},\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Exemplo de criação de uma vector store, possibilitando buscas semânticas em documentos\n",
    "\n",
    "Uma vector store é uma base de dados semântica (ou banco de dados vetorial), que armazena documentos segmentados em forma de vetores. Esses vetores também são conhecidos como embeddings e são gerados por um modelo de Inteligência Artificial (IA) adicional. \n",
    "\n",
    "Existem diversas ferramentas para gerenciar vector stores. Neste exemplo, vamos utilizar a FAISS, uma biblioteca da Meta que possibilita a criação de vector stores localmente, fornecendo funções de busca semântica. Nas células abaixo, vamos criar uma vector store para armazenar um documento pdf, segmentando o texto em chunks de 1000 caracteres.\n",
    "\n",
    "Links úteis:\n",
    "- [FAISS](https://ai.meta.com/tools/faiss/)\n",
    "- [Integração da FAISS com LangChain](https://python.langchain.com/v0.2/docs/integrations/vectorstores/faiss/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paragraphs(rawText):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    return  text_splitter.split_text(rawText)\n",
    "\n",
    "def load_pdfs(documents_path, pdfs):\n",
    "    text_chunks = []\n",
    "\n",
    "    for pdf in pdfs:\n",
    "        reader = PdfReader(f\"{documents_path}/{pdf}\")\n",
    "        for page in reader.pages:\n",
    "            raw = page.extract_text()\n",
    "            chunks = split_paragraphs(raw)\n",
    "            text_chunks += chunks\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nome\\nWesllei Felipe Heckler\\nNome em citações\\nbibliográficas\\nHECKLER, W. F.;FELIPE HECKLER, WESLLEI;HECKLER,\\nWESLLEI;HECKLER, WESLLEI F .;HECKLER, WESLLEI FELIPE\\nLattes iD\\nhttp://lat tes.cnpq.br/5020012898313017Wesllei Felipe Heckler\\nEnder eço par a acessar este CV : http://lat tes.cnpq.br/5020012898313017\\nID Lat tes: 5020012898313017\\nÚltima atual ização do currículo em 09/09/2024\\nWesllei é doutor ando no Programa de Pós-Gr aduação em Computação Aplicada (PPGCA) da Universidade do Vale do Rio dos Sinos\\n(UNISINOS). Possui Mestr ado em Computação Aplicada na mesma universidade (2022) e graduação em Ciência da Computação pela\\nUniversidade Feevale (2019). Atua como Cientista de Dados, desen volvendo soluções de dados através do uso de Ciência de Dados e\\nInteligência Artificial, com experiência no desenvolvimento , validação e monitoramento  de modelos de Machine Learning. Wesllei'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_pdfs = [\"documento.pdf\"]\n",
    "text_chunks = load_pdfs('documentos', list_of_pdfs)\n",
    "text_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "faiss_vector_store = FAISS.from_texts(text_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a vector store criada, podemos realizar um processo de busca semântica, a fim de encontrar segmentos semanticamente similares a um termo. Podemos configurar o número de segmentos retornados para cada busca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Nome\n",
      "Wesllei Felipe Heckler\n",
      "Nome em citações\n",
      "bibliográficas\n",
      "HECKLER, W. F.;FELIPE HECKLER, WESLLEI;HECKLER,\n",
      "WESLLEI;HECKLER, WESLLEI F .;HECKLER, WESLLEI FELIPE\n",
      "Lattes iD\n",
      "http://lat tes.cnpq.br/5020012898313017Wesllei Felipe Heckler\n",
      "Ender eço par a acessar este CV : http://lat tes.cnpq.br/5020012898313017\n",
      "ID Lat tes: 5020012898313017\n",
      "Última atual ização do currículo em 09/09/2024\n",
      "Wesllei é doutor ando no Programa de Pós-Gr aduação em Computação Aplicada (PPGCA) da Universidade do Vale do Rio dos Sinos\n",
      "(UNISINOS). Possui Mestr ado em Computação Aplicada na mesma universidade (2022) e graduação em Ciência da Computação pela\n",
      "Universidade Feevale (2019). Atua como Cientista de Dados, desen volvendo soluções de dados através do uso de Ciência de Dados e\n",
      "Inteligência Artificial, com experiência no desenvolvimento , validação e monitoramento  de modelos de Machine Learning. Wesllei [{}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = faiss_vector_store.similarity_search(\n",
    "    \"Wesllei Heckler\",\n",
    "    k=1\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Vínculo: Celetista, Enquadr amento Funcional: Coordenador\n",
      "de Suporte, Car ga horária: 40\n",
      "Outras informações\n",
      "Coordenação da equipe de suporte técnico , suporte técnico\n",
      "ao cliente e r ealização de visi tas a cl ientes.\n",
      "Vínculo institucional\n",
      "2012 - 2014\n",
      "Vínculo: Celetista, Enquadr amento Funcional: Suporte\n",
      "Técnico , Carga horária: 40\n",
      "Outras informações\n",
      "Suporte técnico para sistemas de departamento pessoal e\n",
      "RH, escri ta fiscal, contabi lidade e gestão empr esarial.\n",
      "Vínculo institucional\n",
      "2018 - 2020\n",
      "Vínculo: Colabor ador, Enquadr amento Funcional:\n",
      "Pesquisador de Aperf eiçoamento Científico\n",
      "Vínculo institucionalUniversidade Feevale, FEEVALE, Brasil.\n",
      "CWI Software LTDA, CWI, Brasil. [{}]\n",
      "\n",
      "* Feevale (INOV AMUNDI), 2018. p . 78-78.\n",
      "1.\n",
      "PFEIFFER SALOMÃO DIAS, LUCA S ; DAMA SCENO VIANNA, HENRIQUE ; FELIPE\n",
      "HECKLER, WESLLEI  ; BARB OSA, JORG E LUIS VICTÓRIA . Identificando Comportamentos\n",
      "de Risco para Doenças Crônicas: Uma abordagem baseada em ontologia . iSys - Revista\n",
      "Brasileira de Sistemas de I nformação , 2024 .Artigos aceitos para publicação\n",
      "Apresentações de Trabalho [{}]\n",
      "\n",
      "* 21.\n",
      "The Dev eloper's Conf erence - T rilha Data S cience II. 2019. (Outr a).\n",
      "22.\n",
      "The Dev eloper's Conf erence - T rilha Machine Learning II. 2019. (Outr a).\n",
      "23.\n",
      "The Dev eloper's Conf erence - T rilha Python. 2019. (Outr a).\n",
      "24.\n",
      "The Dev eloper's Conf erence - T rilha Saúde 4 .0. 2019. (Outr a).\n",
      "25.\n",
      "The Developer's Conference - Trilha Saúde 4.0.Uti lizando Machine Learn ing para prever\n",
      "tendências de abandono em um Pr ograma de R eabilitação Pulmonar . 2019. (Outr a).\n",
      "26.\n",
      "Feira de Iniciação Científica Feevale (INOV AMUNDI). Revisão Sistemática Sobre Técnicas\n",
      "de Anál ise Pr editiva Aplicadas à R eabilitação Pulmonar . 2018. (F eira). [{}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = faiss_vector_store.similarity_search(\n",
    "    \"Feevale\",\n",
    "    k=3\n",
    ")\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Exemplo de interação com um LLM pré-treinado através de uma arquitetura RAG\n",
    "\n",
    "A partir da criação da vector store, podemos automatizar o processo de enriquecimento de contexto ao interagir com LLMs. Na prática, a arquitetura RAG inclui uma camada de busca semântica anterior à interação com o LLM, a fim de buscar documentos possivelmente relevantes para o processamento do prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = faiss_vector_store.as_retriever(search_type=\"mmr\",\n",
    "                                            search_kwargs={\"k\": 1})\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É necessário definir um template de prompt, no qual o system prompt refere-se ao papel que o LLM deve interpretar ao processar o prompt do usuário, seguindo regras específicas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"Você é um assistente para responder perguntas. \"\n",
    "    \"Use os seguintes segmentos de contextos recuperados para responder\"\n",
    "    \"a pergunta. Se você não souber a resposta, diga que não sabe \"\n",
    "    \"Use no máximo três sentenças para responder, a fim de manter\"\n",
    "    \"a resposta concisa.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca LangChain é a mais popular para o desenvolvimento de sistemas baseados em IA Generativa, disponibilizando componentes pré-configurados que facilitam a implementação desses sistemas.\n",
    "\n",
    "Link útil:\n",
    "- [Tutorial de RAG com LangChain](https://python.langchain.com/v0.2/docs/tutorials/rag/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a cadeia criada, agora o sistema busca automaticamente o contexto para responder à nossa pergunta inicial sobre uma pessoa desconhecida para o LLM pré-treinado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:\n",
      "Wesllei Felipe Heckler é um doutorando no Programa de Pós-Graduação em Computação Aplicada da Universidade do Vale do Rio dos Sinos (UNISINOS). Ele possui mestrado em Computação Aplicada e graduação em Ciência da Computação. Atua como Cientista de Dados, focando em soluções de dados e desenvolvimento de modelos de Machine Learning.\n",
      "\n",
      "Contexto:\n",
      "Nome\n",
      "Wesllei Felipe Heckler\n",
      "Nome em citações\n",
      "bibliográficas\n",
      "HECKLER, W. F.;FELIPE HECKLER, WESLLEI;HECKLER,\n",
      "WESLLEI;HECKLER, WESLLEI F .;HECKLER, WESLLEI FELIPE\n",
      "Lattes iD\n",
      "http://lat tes.cnpq.br/5020012898313017Wesllei Felipe Heckler\n",
      "Ender eço par a acessar este CV : http://lat tes.cnpq.br/5020012898313017\n",
      "ID Lat tes: 5020012898313017\n",
      "Última atual ização do currículo em 09/09/2024\n",
      "Wesllei é doutor ando no Programa de Pós-Gr aduação em Computação Aplicada (PPGCA) da Universidade do Vale do Rio dos Sinos\n",
      "(UNISINOS). Possui Mestr ado em Computação Aplicada na mesma universidade (2022) e graduação em Ciência da Computação pela\n",
      "Universidade Feevale (2019). Atua como Cientista de Dados, desen volvendo soluções de dados através do uso de Ciência de Dados e\n",
      "Inteligência Artificial, com experiência no desenvolvimento , validação e monitoramento  de modelos de Machine Learning. Wesllei\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Quem é Wesllei Heckler?\"})\n",
    "print('Resposta:')\n",
    "print(response[\"answer\"])\n",
    "\n",
    "print()\n",
    "print('Contexto:')\n",
    "for document in response[\"context\"]:\n",
    "    print(document.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conforme as instruções definidas no system_prompt, o sistema indica quando não possui informações suficientes para responder uma pergunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:\n",
      "Não sei onde Wesllei Heckler mora.\n",
      "\n",
      "Contexto:\n",
      "Nome\n",
      "Wesllei Felipe Heckler\n",
      "Nome em citações\n",
      "bibliográficas\n",
      "HECKLER, W. F.;FELIPE HECKLER, WESLLEI;HECKLER,\n",
      "WESLLEI;HECKLER, WESLLEI F .;HECKLER, WESLLEI FELIPE\n",
      "Lattes iD\n",
      "http://lat tes.cnpq.br/5020012898313017Wesllei Felipe Heckler\n",
      "Ender eço par a acessar este CV : http://lat tes.cnpq.br/5020012898313017\n",
      "ID Lat tes: 5020012898313017\n",
      "Última atual ização do currículo em 09/09/2024\n",
      "Wesllei é doutor ando no Programa de Pós-Gr aduação em Computação Aplicada (PPGCA) da Universidade do Vale do Rio dos Sinos\n",
      "(UNISINOS). Possui Mestr ado em Computação Aplicada na mesma universidade (2022) e graduação em Ciência da Computação pela\n",
      "Universidade Feevale (2019). Atua como Cientista de Dados, desen volvendo soluções de dados através do uso de Ciência de Dados e\n",
      "Inteligência Artificial, com experiência no desenvolvimento , validação e monitoramento  de modelos de Machine Learning. Wesllei\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Onde Wesllei Heckler mora?\"})\n",
    "print('Resposta:')\n",
    "print(response[\"answer\"])\n",
    "\n",
    "print()\n",
    "print('Contexto:')\n",
    "for document in response[\"context\"]:\n",
    "    print(document.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que o sistema segue aproveitando o conhecimento obtido pelo LLM durante o seu treinamento. Apesar de não termos documentos sobre Alan Turing na vector store criada, o sistema ainda é capaz de responder sobre quem é Alan Turing. \n",
    "\n",
    "Também é possível notar que, ainda assim, o sistema considera um segmento de documento para responder a pergunta. Porém, o LLM ignora esse contexto ao entender que o mesmo não é relevante para a resposta final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta:\n",
      "Alan Turing foi um matemático, lógico e criptógrafo britânico, considerado um dos pais da ciência da computação e da inteligência artificial. Ele é conhecido por desenvolver a máquina de Turing, um conceito fundamental na teoria da computação, e por seu trabalho na quebra de códigos durante a Segunda Guerra Mundial, que ajudou a decifrar a máquina Enigma. Turing também fez contribuições significativas para a compreensão da computação e do que significa ser \"inteligente\".\n",
      "\n",
      "Contexto:\n",
      "Inteligência Artificial, com experiência no desenvolvimento , validação e monitoramento  de modelos de Machine Learning. Wesllei\n",
      "também é pesquisador voluntário no Laboratório de Computação Móvel (Mobi lab) da Universidade do Vale do Rio dos Sinos\n",
      "(UNISINOS), onde colabor a em pesquisas que envolvem a aplicação de tecnologias na área da saúde, principalmente envolvendo\n",
      "Machine Learning. Atualmente pesquisa modelos de Machine Learning e Inteligência Artificial Gener ativa para a área de Saúde\n",
      "Mental. Seus principais interesses de pesquisa são Data Science, Machine Learning e tecnologia aplicados à Saúde Mental.  (Texto\n",
      "informado pelo autor)\n",
      "Identificação\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Quem é Alan Turing?\"})\n",
    "print('Resposta:')\n",
    "print(response[\"answer\"])\n",
    "\n",
    "print()\n",
    "print('Contexto:')\n",
    "for document in response[\"context\"]:\n",
    "    print(document.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
